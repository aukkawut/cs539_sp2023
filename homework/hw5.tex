\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{fancyvrb}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[super]{nth}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{mathrsfs}
\usepackage{tzplot}
\usepackage{pgfplots, tikz}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{titlesec}
\usepackage{lmodern}
\usepackage{etoolbox}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{listings}
\usepackage{bm}
%\usepackage{lipsum}

\makeatletter
\patchcmd{\section}{-3.5ex \@plus -1ex \@minus -.2ex}{-3.5ex \@plus -1ex \@minus -.2ex\setlength{\leftskip}{0cm}}{}{}
\patchcmd{\subsection}{-3.25ex\@plus -1ex \@minus -.2ex}{3.25ex\@plus -1ex \@minus -.2ex\setlength{\leftskip}{0cm}}{}{}
\patchcmd{\subsection}{1.5ex \@plus .2ex}{1.5ex \@plus .2ex\setlength{\leftskip}{2cm}}{}{}
\makeatother
\titleformat{\section}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesection}{1em}{}
\pagestyle{fancy}
\fancyhf{}
\rhead{Aukkawut Ammartayakun}
\lhead{CS 539 Machine Learning: Homework 5}
\cfoot{\thepage}
\title{Homework 3}
\author{Aukkawut Ammartayakun\\CS 539 Machine Learning}
\date{Spring 2023}
\newcommand{\vtt}[1]{%
  \text{\normalfont\ttfamily\detokenize{#1}}%
}



\newcommand{\indep}{\perp \!\!\! \perp}
\begin{document}
\newcommand{\fakesection}[1]{%
  \par\refstepcounter{section}% Increase section counter
  \sectionmark{#1}% Add section mark (header)
  \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}% Add section to ToC
  % Add more content here, if needed.
}
\newcommand{\fakesubsection}[1]{%
  \par\refstepcounter{subsection}% Increase subsection counter
  \subsectionmark{#1}% Add subsection mark (header)
  \addcontentsline{toc}{subsection}{\protect\numberline{\thesubsection}#1}% Add subsection to ToC
  % Add more content here, if needed.
}
\theoremstyle{definition}
\newtheorem*{sol}{Solution}
\maketitle
\fakesection{q1}
\noindent
\Large{\textbf{Problem 1}}\normalsize
\\

Evaluate the Kullback-Leibler divergence (equation 1.113 in Bishop) 
\[ \text{KL}(p||q) = -\int p(x)\ln\left[\frac{p(x)}{q(x)}\right]\; dx\]
between two Gaussians
$p(x) = \mathcal{N}(x|\mu, \Sigma)$ and $q(x) = \mathcal{N}(x|m,L)$.

\color{blue}
\begin{sol}
  Assume that $p$ and $q$ are $n$ dimensional gaussian;
  \begin{align*}
    \int p(x) \ln\left[\frac{p(x)}{q(x)}\right] \; dx &= \frac{1}{2}\left(\ln\frac{|\Sigma|}{|L|} - \mathbb{E}[(x-\mu)^\top\Sigma^{-1}(x-\mu)] + \mathbb{E}[(x-m)^\top L^{-1}(x-m)]\right) \\
    &= \frac{1}{2}\left[\ln\left(\frac{|\Sigma|}{|L|} - n + (\mu-m)^\top \Sigma^{-1} (\mu - m) + \text{tr}(L^{-1}\Sigma)\right)\right]
  \end{align*}
\end{sol}
\color{black}
\leavevmode\\
\fakesection{q2}
\noindent
\Large{\textbf{Problem 2}}\normalsize
\\

Suppose that $p(x)$ is some fixed distribution and that we wish to approximate it using a Gaussian
distribution $q(x) =  \mathcal{N}(x|\mu, \Sigma)$. By writing down the form of the KL divergence $\text{KL}(p||q)$ for a
Gaussian $q(x)$ and then differentiating, show that minimization of $\text{KL}(p||q)$ with respect to $\mu$ and $\Sigma$
leads to the result that $\mu$ is given by the expectation of $x$ under $p(x)$ and that $\Sigma$ is given by the
covariance.

\color{blue}
\begin{sol}
    Given that $q(x)$ is gaussian,
    \begin{align*}
      \text{KL}(p||q) = \mathbb{E}_p[\ln q(x)] + k
    \end{align*}
    for some constant $k$ that represent negative entropy of $p(x)$. Differentiate this and we will get that $\mu$ is given by the expectation of $x$ under $p(x)$ and that $\Sigma$ is given by the covariance by default.
\end{sol}
\color{black}
\leavevmode\\
\fakesection{q3}
\noindent
\Large{\textbf{Problem 3}}\normalsize
\\

Consider a regression problem involving multiple target variables in which it is assumed that the
distribution of the targets, conditioned on the input vector $x$, is a Gaussian of the form
\[p(t|x,w) = \mathcal{N}(t|y(x,w), \Sigma)\]
where $y(x,w)$ is the output of a neural network with input vector $x$ and weight vector $w$, and $\Sigma$ is
the covariance of the assumed Gaussian noise on the targets.

Given a set of independent observations of $x$ and $t$, write down the error function that must be
minimized in order to find the maximum likelihood solution for $w$, if we assume that $\Sigma$ is fixed
and known. Now assume that $\Sigma$ is also to be determined from the data, and write down an
expression for the maximum likelihood solution for $\Sigma$. Note that the optimizations of $w$ and $\Sigma$ are
now coupled, in contrast to the case of independent target variables discussed in Section 5.2.
\color{blue}
\begin{sol}
Assume that $y$ and $t$ is $k$ dimensional vector, log-likelihood in this case is
\begin{align*}
  \ln p(t|x,w) &= -\frac{N}{2}\ln|\Sigma| - \frac{1}{2}(t-y(x,w))^\top\Sigma^{-1}(t-y(x,w)) - \frac{Nk}{2}\ln(2\pi)
\end{align*}
Using the profiling likelihood, fixed $w$. Evaluate the derivative with respect to $\Sigma$ and set it to zero will yield use
\[\Sigma = \frac{1}{N}\sum_{i=1}^{N} (t_i-y_i(x_i,w))(t_i-y_i(x_i,w))^\top\]
\end{sol}
\color{black}
\leavevmode\\
\fakesection{q4}
\noindent
\Large{\textbf{Problem 4}}\normalsize
\\

Let $z = e^{x^2y}$ where $x(u;v) = \sqrt{uv}$ and $y(u;v) = \frac{1}{v}$.
\color{black}
\begin{enumerate}
  \item Derive $\frac{\partial z}{\partial u}$ and $\frac{\partial z}{\partial v}$
  \color{blue}
  \begin{sol}
    \begin{align*}
    \frac{\partial z}{\partial u} &= \frac{\partial z}{\partial x} \frac{\partial x}{\partial u} +  \frac{\partial z}{\partial y} \frac{\partial y}{\partial u}\\
    &= 2xye^{x^2y} \frac{v}{2\sqrt{uv}}\\
    \frac{\partial z}{\partial v} &= \frac{\partial z}{\partial x} \frac{\partial x}{\partial v} +  \frac{\partial z}{\partial y} \frac{\partial y}{\partial v}\\
    &= 2xye^{x^2y} \frac{v}{2\sqrt{uv}} + x^2e^{x^2y} \frac{-1}{v^2}
    \end{align*}
  \end{sol}
  \color{black}
  \item Letâ€™s assume the target value for the output ($z$) is $t$. We want to minimize the $e = \frac{1}{2}(t-z)^2$; write down the update rule for changing $u$ and $v$ that minimizes $e$.
  \color{blue}
  \begin{sol}
    The graph is as followed
    $u,v \rightarrow x,y \rightarrow z \rightarrow e$
    Evaluate $\frac{\partial e}{\partial u} = -(t-z)\frac{\partial z}{\partial u}$ and $\frac{\partial e}{\partial v} = -(t-z)\frac{\partial z}{\partial v}$.
    As shown in the first part, we can use gradient descent to update $u$ and $v$ with the gradient from error calculated aboved. That is
    \begin{align*}
      u_{n+1} &= u_n - \alpha \frac{\partial e}{\partial u}\\
      v_{n+1} &= v_n - \alpha \frac{\partial e}{\partial v}
    \end{align*}
    for constant $\alpha$.
    
  \end{sol}
  \color{black} 
\end{enumerate}
\end{document}

