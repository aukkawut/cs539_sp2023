\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{fancyvrb}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{mathrsfs}
\usepackage{tzplot}
\usepackage{tikz}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{titlesec}
\usepackage{lmodern}
\usepackage{etoolbox}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage[shortlabels]{enumitem}
%\usepackage{lipsum}

\makeatletter
\patchcmd{\section}{-3.5ex \@plus -1ex \@minus -.2ex}{-3.5ex \@plus -1ex \@minus -.2ex\setlength{\leftskip}{0cm}}{}{}
\patchcmd{\subsection}{-3.25ex\@plus -1ex \@minus -.2ex}{3.25ex\@plus -1ex \@minus -.2ex\setlength{\leftskip}{0cm}}{}{}
\patchcmd{\subsection}{1.5ex \@plus .2ex}{1.5ex \@plus .2ex\setlength{\leftskip}{2cm}}{}{}
\makeatother
\titleformat{\section}
  {\normalfont\fontsize{12}{15}\bfseries}{\thesection}{1em}{}
\pagestyle{fancy}
\fancyhf{}
\rhead{Aukkawut Ammartayakun}
\lhead{CS 539 Machine Learning: Homework 1}
\cfoot{\thepage}
\title{Homework 1}
\author{Aukkawut Ammartayakun\\CS 539 Machine Learning}
\date{Spring 2023}
\newcommand{\vtt}[1]{%
  \text{\normalfont\ttfamily\detokenize{#1}}%
}



\newcommand{\indep}{\perp \!\!\! \perp}
\begin{document}
\newcommand{\fakesection}[1]{%
  \par\refstepcounter{section}% Increase section counter
  \sectionmark{#1}% Add section mark (header)
  \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}% Add section to ToC
  % Add more content here, if needed.
}
\newcommand{\fakesubsection}[1]{%
  \par\refstepcounter{subsection}% Increase subsection counter
  \subsectionmark{#1}% Add subsection mark (header)
  \addcontentsline{toc}{subsection}{\protect\numberline{\thesubsection}#1}% Add subsection to ToC
  % Add more content here, if needed.
}
\theoremstyle{definition}
\newtheorem*{sol}{Solution}
\maketitle
\fakesection{q1}
\noindent
\Large{\textbf{Problem 1}}\normalsize
\\

Consider two variables $X$ and $Y$ with joint distribution $p(x, y)$. Prove the following two results
\begin{enumerate}
    \item $\mathbb{E}[X] = \mathbb{E}_Y\left[\mathbb{E}_X[X|Y]\right]$
    \color{blue}
    \begin{sol}
        Consider the RHS,
\begin{align*}
    \mathbb{E}_Y\left[\mathbb{E}_X[X|Y]\right] &= \int_{\Omega_Y}f_Y(y)\;dy\\
    &= \int_{\Omega_Y}\int_{\Omega_X}xf_{X|Y}(x|y)f_Y(y)\;dx\;dy\\
    &= \int_{\Omega_Y}\int_{\Omega_X}xp(x,y)\;dx\;dy\\
        &= \int_{\Omega_X}\int_{\Omega_Y}xp(x,y)\;dy\;dx\\
        &= \int_{\Omega_X} xf_{X}(x)\;dx\\
            &= \mathbb{E}[X]
\end{align*}
    \end{sol}
    \color{black}
    \item $\text{Var}[X] = \mathbb{E}_Y\left[\text{Var}_X[X|Y]\right] + \text{Var}_Y\left[\mathbb{E}_X[X|Y]\right]$
    \color{blue}
    \begin{sol}
        Consider the LHS,
\begin{align*}
    \text{Var}[X] &= \mathbb{E}[X^2] - \mathbb{E}^2[X] \\
    &= \mathbb{E}\left[\text{Var}[X|Y]+ \mathbb{E}^2[X|Y]\right]- \mathbb{E}^2[\mathbb{E}[X|Y]]\\
    &= \mathbb{E}\left[\text{Var}[X|Y]\right] + \mathbb{E}[\mathbb{E}^2[X|Y]]- \mathbb{E}^2[\mathbb{E}[X|Y]]\\
    &= \mathbb{E}_Y\left[\text{Var}_X[X|Y]\right] + \text{Var}_Y\left[\mathbb{E}_X[X|Y]\right]
\end{align*}
\end{sol}
\color{black}
\end{enumerate}
\leavevmode\\
\fakesection{q2}
\noindent
\Large{\textbf{Problem 2}}\normalsize
\\

Let $X$ and $Z$ be two independent random vectors, so that $p(X, Z) = p(X)p(Z)$. Show that the mean of their sum $Y = X+Z$ is given by the sum of the means of each of the variables separately. Similarly, show that the covariance matrix of $Y$ is given by the sum of the covariance matrices of $X$ and $Z$.
\color{blue}
\begin{sol}
    Consider the mean of joint distribution $Y = X+Z$,
\begin{align*}
    \mathbb{E}[Y] &= \int_{\Omega_X}\int_{\Omega_Z}yf_{Y}(y)\;dy\;dx\\
    &= \int_{\Omega_X}\int_{\Omega_Z}(x+z)f_{X}(x)f_{Z}(z)\;dx\;dz\\
    &= \int_{\Omega_X}\int_{\Omega_Z}xf_{X}(x)f_{Z}(z)\;dx\;dz + \int_{\Omega_X}\int_{\Omega_Z}zf_{X}(x)f_{Z}(z)\;dx\;dz\\
    &= \int_{\Omega_X}xf_{X}(x)\;dx + \int_{\Omega_Z}zf_{Z}(z)\;dz\\
    &= \mathbb{E}[X] + \mathbb{E}[Z]
\end{align*}
And also consider the covariance matrix of joint distribution $Y = X+Z$,
\begin{align*}
    \text{cov}[Y] &= \mathbb{E}\left[\left(Y-\mathbb{E}[Y]\right)\left(Y-\mathbb{E}[Y]\right)^T\right]\\
    &= \mathbb{E}\left[\left(X+Z-\mathbb{E}[X]-\mathbb{E}[Z]\right)\left(X+Z-\mathbb{E}[X]-\mathbb{E}[Z]\right)^T\right]\\
    &= \mathbb{E}\left[\left(X-\mathbb{E}[X]\right)\left(X-\mathbb{E}[X]\right)^T\right] + \mathbb{E}\left[\left(Z-\mathbb{E}[Z]\right)\left(Z-\mathbb{E}[Z]\right)^T\right]\\
    &= \text{cov}[X] + \text{cov}[Z]
\end{align*}
\end{sol}
\color{black}
\leavevmode\\
\fakesection{q3}
\noindent
\Large{\textbf{Problem 3}}\normalsize
\\

Consider a $D$-dimensional Gaussian random variable $X$ with distribution $\mathcal{N}(x|\mu,\Sigma)$ in which the covariance $\Sigma$ is known and for which we wish to infer the mean $\mu$ from a set of observations $\mathcal{X} = \left\{X_1,\dots,X_n\right\}$. Given a prior distribution $p(\mu) =\mathcal{N}\left(\mu|\mu_0,\Sigma_0\right) $, find the corresponding posterior distribution $p(\mu|\mathcal{X})$

\color{blue}
\begin{sol}
    The likelihood of the multivariate Gaussian distribution is given by,
\begin{align*}
\mathcal{L}(\mu|X,\Sigma) &= \left(\prod_{i=1}^n \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp\left((X_i-\mu)^\top\Sigma^{-1}(X_i-\mu)\right)\right) \\
&= \frac{1}{(2\pi)^{Dn/2}|\Sigma|^{n/2}} \exp\left(-\frac{1}{2}\sum_{i=1}^n(X_i-\mu)^\top\Sigma^{-1}(X_i-\mu)\right)
\end{align*}
with prior,
\begin{align*}
p(\mu|\mu_0,\Sigma_0) &= \frac{1}{(2\pi)^{D/2}|\Sigma_0|^{1/2}} \exp\left(-\frac{1}{2}(\mu-\mu_0)^\top\Sigma_0^{-1}(\mu-\mu_0)\right)
\end{align*}
The posterior distribution would be
\begin{align*}
p(\mu|\mathcal{X},\mu_0,\Sigma_0) &\propto \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mu-\mu_0)^\top\Sigma_0^{-1}(\mu-\mu_0)\right)\frac{1}{(2\pi)^{Dn/2}|\Sigma|^{n/2}} \exp\left(-\frac{1}{2}\sum_{i=1}^n(X_i-\mu)^\top\Sigma^{-1}(X_i-\mu)\right)\\
&= \frac{1}{(2\pi)^{(n+1)D/2}\left(|(\Sigma)^n\Sigma_0|\right)^{1/2}} \exp\left(-\frac{1}{2}\left[(\mu-\mu_n)^\top\Sigma_0^{-1}(\mu-\mu_n) +\sum_{i=1}^n(X_i-\mu)^\top\Sigma^{-1}(X_i-\mu) \right]\right)
\end{align*}
which resemble the multivariate Gaussian distribution.
\end{sol}
\color{black}
\leavevmode\\
\fakesection{q4}
\noindent
\Large{\textbf{Problem 4}}\normalsize
\\

Consider a univariate Gaussian distribution $\mathcal{N}(x|\mu, \tau^{-1})$ having conjugate Gaussian-gamma prior given by 
$$p(\mu,\lambda) = \mathcal{N}\left(\mu|\mu_0,(\beta\lambda)^{-1}\right)\text{Gamma}(\lambda|a,b)$$
and a dataset $\mathcal{X} = \left\{X_1,\dots,X_n\right\}$ of i.i.d. observations. Show that the posterior distribution is 
also a Gaussian-gamma distribution of the same functional form as the prior, and write down 
expressions for the parameters of this posterior distribution.

\color{blue}
\begin{sol}
    Consider the fact that the conjugate prior of the Gaussian distribution is the Gaussian-gamma distribution. The posterior distribution would be in the same family due to the fact that
    $$\text{posterior} = c\times\text{conjugate prior} \propto \text{(likelihood)(prior)}$$
    One can show this by evaluating the likelihood with Gamma prior.
    \begin{align*}
    \mathcal{L}(\mu,\lambda|\mathcal{X})P(\mu,\lambda|a,b) &= \left(\prod_{i=1}^n \frac{1}{\sqrt{2\pi\tau^{-1}}} \exp\left(-\frac{\tau}{2}(X_i-\mu)^2\right)\right)\frac{\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)}\\
    &= \frac{1}{(2\pi\tau^{-1})^{n/2}\Gamma(a)} \exp\left(-\frac{\tau}{2}\sum_{i=1}^n(X_i-\mu)^2\right)\lambda^{a-1}e^{-b\lambda}
    \end{align*}
    Notice that if we were to substitute $\tau$ with $\beta\lambda$, we will get
    \begin{align*}
    \mathcal{L}(\mu,\lambda|\mathcal{X})P(\mu,\lambda|a,b) &= \frac{(\beta\lambda)^{n/2}}{(2\pi)^{n/2}\Gamma(a)} \exp\left(-\frac{\beta\lambda}{2}\sum_{i=1}^n(X_i-\mu)^2\right)\lambda^{a-1}e^{-b\lambda}
    \end{align*}
    which is basically Gaussian-gamma distribution. 
\end{sol}
\color{black}
\leavevmode\\
\newpage
\fakesection{q5}
\noindent
\Large{\textbf{Problem 5}}\normalsize
\\

Show that if two variables $X$ and $Y$ are independent, then their covariance is zero. 
\color{blue}
\begin{lemma}
\label{lemma:indepexp}
Let $X$ and $Y$ be random variable where $X\indep Y$, then $\mathbb{E}[XY] =  \mathbb{E}[X] \mathbb{E}[Y]$
\end{lemma}
\begin{proof}
    Assume that $X\sim \mu(x)$ and $Y\sim \nu(y)$ where $\mu:\Omega_X\to\mathcal{B}_X$ and $\nu:\Omega_Y\to\mathcal{B}_Y$ where $\mathcal{B}$ is a Borel field, the expectation operator is defined as
    $$\mathbb{E}[X] = \int_{\Omega_X} x\;d\mu(x)$$
    Using the Fubini-Tonelli theorem, we can show that
    \begin{align*}
    \mathbb{E}[XY] &= \int_{\Omega_X}\int_{\Omega_Y} xy\;d\pi(x,y)\\
    &= \int_{\Omega_X} x\;d\mu(x)\int_{\Omega_X} y\;d\nu(y)\\
    &= \mathbb{E}[X]\mathbb{E}[Y]
    \end{align*}
\end{proof}
\begin{sol}
We will show that if $X\indep Y$, then $\text{cov}(X,Y) = 0$.
Recall that the definition of covariance is 
\begin{equation}\text{cov}(X,Y) = \mathbb{E}\left[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])\right]\end{equation}
From this, we can simplify the covariance as
\begin{align*}
    \text{cov}(X,Y) &= \mathbb{E}\left[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])\right] \\
     &= \mathbb{E}\left[XY-Y\mathbb{E}[X]Y - X\mathbb{E}[Y]  +\mathbb{E}[X] \mathbb{E}[Y])\right] \\
     &= \mathbb{E}[XY] - \mathbb{E}[Y]\mathbb{E}[X] - \mathbb[X]\mathbb{E}[Y]  +\mathbb{E}[X] \mathbb{E}[Y]) \\
     &= \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y]
\end{align*}
Since, $X\indep Y$. That implies $\mathbb{E}[XY] =  \mathbb{E}[X] \mathbb{E}[Y]$ (from lemma \ref{lemma:indepexp}). In other words, $\text{cov}(X,Y) = 0$.
\end{sol}
\color{black}
\leavevmode\\
\fakesection{q6}
\noindent
\Large{\textbf{Problem 6}}\normalsize
\\

Evaluate the Kullback-Leibler divergence between two Gaussians $p(x) = \mathcal{N}(x|\mu, \sigma^2)$ and $q(x) = \mathcal{N}(x|m, s^2)$ 
\color{blue}
\begin{sol}
    Recall the definition of KL divergence between two measures $p(x)$ and $q(x)$ given that $p,q: \mathbb{R}\rightarrow \mathcal{B}$ where $\mathcal{B}$ is a Borel field.
    \begin{equation}D_{KL}(p(x) || q(x))  = \int_{-\infty}^{\infty} p(x) \ln\left(\frac{p(x)}{q(x)}\right) \; dx\end{equation}
In this case, 
\begin{align*}
    D_{KL}(p(x)||q(x)) &= \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}  \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \ln\left(\frac{\frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)}{\frac{1}{\sqrt{2\pi}s} \exp\left(-\frac{(x-m)^2}{2s^2}\right)}\right)\; dx \\
    &=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}  \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \left(\ln\left(\frac{s}{\sigma}\right) + \left[\frac{(x-m)^2}{2s^2} - \frac{(x-\mu)^2}{2\sigma^2}\right]\right) \; dx \\
    &= \ln\left(\frac{s}{\sigma}\right) + \frac{1}{\sqrt{2\pi}\sigma}\left[\int_{-\infty}^{\infty}  \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \frac{(x-m)^2}{2s^2} \; dx -\int_{-\infty}^{\infty} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \frac{(x-\mu)^2}{2\sigma^2}\; dx\right] \\
    &= \ln\left(\frac{s}{\sigma}\right) - \frac{1}{2} + \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}  \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \frac{(x-m)^2}{2s^2} \; dx \\
    &= \ln\left(\frac{s}{\sigma}\right) - \frac{1}{2} + \frac{(\mu - m)^2 + \sigma^2}{s^2}
\end{align*}
\end{sol}
\color{black}
\newpage
\fakesection{q7}
\noindent
\Large{\textbf{Problem 7}}\normalsize
\\

For Inverse Gamma distribution 
$$f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha-1}\exp\left(-\frac{\beta}{x}\right)$$
\begin{enumerate}
    \item Show the mean of the distribution is
    $$\mathbb{E}[X] = \frac{\beta}{\alpha - 1}$$
    \color{blue}
    \begin{sol}
    We want to show that $\mathbb{E}[X] = \frac{\beta}{\alpha - 1}$. Using the definition of expectation and the fact that 
    \begin{equation}
            \Gamma(\alpha) = \int_0^{\infty} x^{\alpha-1} \exp(-x) \; dx 
    \end{equation}
    Let $u = \frac{\beta}{x}$
    \begin{align*}
        \mathbb{E}[X] &= \frac{\beta^\alpha}{\Gamma(\alpha)}\beta^{-\alpha +1}\int_{0}^{\infty} u^{\alpha - 2}\exp(-u) du \\
        & = \frac{\beta^\alpha}{\Gamma(\alpha)}\beta^{-\alpha + 1}\Gamma(\alpha-1) \\
        & = \frac{\beta}{\alpha - 1}
    \end{align*}
    \end{sol}
    \color{black}
    \item Find maximum likelihood estimate of $\alpha$ and $\beta$ given $N$ independent sample of $x_i$
    \color{blue}
    \begin{sol}
        The maximum likelihood can be evaluated from its likelihood function. Namely,
    \begin{equation}
        \mathcal{L}(\alpha,\beta|X) = \prod_{i=1}^{N} f(x_i|a,b)
    \end{equation}
    In this case, the likelihood function is
    \begin{align*}
        \mathcal{L}(\alpha,\beta|X) &= \left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)^n\exp\left(-\sum_{i=1}^N\frac{\beta}{x_i}\right) \prod_{k=1}^{N}x^{-(\alpha+1)}_k 
    \end{align*}
    Minimize the log-likelihood with respect to $\alpha$ while fixing $\beta$ 
        \begin{align*}
        \dfrac{\partial}{\partial \alpha}\ln\left(\mathcal{L}(\alpha,\beta|X)\right) &= \dfrac{\partial}{\partial \alpha}\ln\left[\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)^N\exp\left(-\sum_{i=1}^N\frac{\beta}{x_i}\right) \prod_{k=1}^{N}x^{-(\alpha+1)}_k\right]\\
        &=\dfrac{\partial}{\partial \alpha} \left[N\ln\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right) -\sum_{i=1}^{N}\frac{\beta}{x_i} + \sum_{i=1}^N x_k^{-(\alpha + 1)}\right]\\
        &=\dfrac{\partial}{\partial \alpha} \left[N{\alpha}\ln(\beta)- N\ln(\Gamma(\alpha))+\sum_{k=1}^N x_k^{-(\alpha + 1)}\right]\\ 
        &= N\ln(\beta) - N\psi(\alpha) + \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k))
    \end{align*}
    and with respect to $\beta$ while fixing $\alpha$
    \begin{align*}
        \dfrac{\partial}{\partial \beta} \left[N{\alpha}\ln(\beta)- N\ln(\Gamma(\alpha)) - \sum_{i=1}^N\frac{\beta}{x_i} \right] &= \frac{N\alpha}{\beta} - \sum_{i=1}^N \frac{1}{x_i}
    \end{align*}
    Now, we can set the partial derivative to zero and solve for $\alpha$ and $\beta$.
    \begin{align}
         N\ln(\beta) - N\psi(\alpha) + \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= 0 \label{eq:l1}\\
         \frac{N\alpha}{\beta} - \sum_{i=1}^N \frac{1}{x_i} &= 0 \label{eq:l2}
    \end{align}
    From the equation \ref{eq:l2}, we can see that $\beta = \frac{N\alpha}{\sum_{i=1}^N \frac{1}{x_i}}$. Substituting this into equation \ref{eq:l1}, we can solve for $\alpha$.
    That is,
    \begin{align*}
        N\ln\left(\frac{N\alpha}{\sum_{i=1}^N \frac{1}{x_i}}\right) - N\psi(\alpha) + \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= 0 \\
        N\ln\left(\frac{N}{\sum_{i=1}^N \frac{1}{x_i}}\right) - N\psi(\alpha) + \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= 0 \\
        \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= N\psi(\alpha) - N\ln\left(\frac{N}{\sum_{i=1}^N \frac{1}{x_i}}\right) \\
        \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= N\psi(\alpha) - N\ln(N) + N\ln\left(\sum_{i=1}^N \frac{1}{x_i}\right) \\
        \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= N\psi(\alpha) - N\ln(N) + N\ln\left(\frac{1}{\frac{1}{N}\sum_{i=1}^N \frac{1}{x_i}}\right) \\
        \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= N\psi(\alpha) - N\ln(N) + N\ln\left(\frac{N}{\sum_{i=1}^N \frac{1}{x_i}}\right) \\
        \sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) &= N\psi(\alpha) + N\ln\left(\sum_{i=1}^N \frac{1}{x_i}\right)\\
        \psi(\alpha) &= \frac{1}{n}\sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) - \ln(\sum_{i=1}^N \frac{1}{x_i}) \\
        \alpha &= \psi^{-1}\left( \frac{1}{n}\sum_{k=1}^N (-x_k^{-(\alpha+1)}\ln(x_k)) - \ln(\sum_{i=1}^N \frac{1}{x_i})\right)
    \end{align*}
    where $\psi^{-1}$ is the inverse of the digamma function.
    \end{sol}
    \color{black}
    \item Discuss if $f(x)$ follows an exponential family distribution or not. 
    \color{blue}
    \begin{sol}
    To show that $f(x|\theta)$ is the exponential family, we need to show that
    $$f(x|\theta) = h(x)c(\theta) \exp(T(x)\tau(\theta))$$
    From the definition of $f(x)$, we can expand it as
    \begin{align*}
        f(x) &= \frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha-1}\exp\left(-\frac{\beta}{x}\right) \\
        &= \frac{\beta^\alpha}{\Gamma(\alpha)}\exp\left(\ln(x^{-\alpha-1})\right)\exp\left(-\frac{\beta}{x}\right)\\
        &=  \frac{\beta^\alpha}{\Gamma(\alpha)} \exp\left((-\alpha-1)\ln(x) - \frac{\beta}{x}\right)
    \end{align*}
    In this case, $T(x) = \begin{bmatrix}\ln(x) & \frac{1}{x}\end{bmatrix}$ and $\tau(\theta) = \begin{bmatrix}
        -\alpha - 1 & \beta
    \end{bmatrix}$, $c(\theta) =\frac{\beta^\alpha}{\Gamma(\alpha)}$ and $h(x) = 1$. Thus, $f(x)$ is in the exponential family.
    \end{sol}
    \color{black}
    \item Discuss if $X$ is an inverse gamma distribution with $\alpha =1$  and $\beta =c$ , then $Y=1/X$ is an exponential distribution with rate $c$ 
    \color{blue}
    \begin{sol}
        Using the CDF method, one can come to conclude that $P(X\geq 1/y) = 1-F(\frac{1}{y})$ for $Y = y$ and $Y = 1/X$. Thus, evaluate its derivative and we can get
        $$g(y) = \frac{1}{y^2}f\left(\frac{1}{y}\right) = \frac{1}{y^2}\frac{\beta^\alpha}{\Gamma(\alpha)}y^{\alpha+1}\exp\left(-\beta y\right)$$
        Let $\alpha = 1, \beta = c$ we will get exponential PDF.
        $$g(y) = c\exp(-cy)$$
    \end{sol}
    \color{black}
    \item For distribution $Y$, and with a prior on $c$ define the posterior of $c$. Pick a proper prior for $c$ 
first. 
\color{blue}
\begin{sol}
    Using gamma prior, one can get the posterior of $c$ from conjugate prior,
    \begin{align*}
        P(c|Y) &\propto P(Y|c)P(c) \\
        &= c^{n+\alpha - 1}\exp\left(-c\left(\left[\sum_{i=1}^n y\right] - \beta\right)\right)\\
        P(c|Y) &\sim \text{Gamma}\left(n+\alpha, \left[\sum_{i=1}^n y\right] - \beta\right)
    \end{align*}
That is, the posterior of $c$ would be a gamma distribution with parameters $n+\alpha$ and $\left[\sum_{i=1}^n y\right] - \beta$.
\end{sol}
\color{black}
\end{enumerate}
\end{document} 
